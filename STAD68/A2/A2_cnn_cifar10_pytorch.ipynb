{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R09qiAPulJRg"
      },
      "source": [
        "# Convolution Neural Networks: CIFAR-10 Dataset\n",
        "### Please let me know if you found any errors, Email me at qiang.sun@utoronto.ca\n",
        "#### I strongly encourage to buy Google Colab Pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGCX37vKlJRh"
      },
      "source": [
        "This assignment uses the classic [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which is a labeled subset of the 80 million tiny images dataset. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2qzWg7kADxD"
      },
      "outputs": [],
      "source": [
        "###checking if we are getting GPU in colab\n",
        "###if not, Runtime â†’ Change runtime type and Set Hardware accelerator to GPU\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PdcG8-IolJRi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT8FD9t8lJRi"
      },
      "source": [
        "# We are gonna to define all the functions so that we can train the model better later\n",
        "### Data Loader and evaluation [Fill in where you have raise NotImplementedError(\"Fill me in\")]\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5DQuc7vdlJRj"
      },
      "outputs": [],
      "source": [
        "### data loaders (CIFAR-10) ####\n",
        "def get_cifar10_loaders(batch_size=128, num_workers=2, val_size=5000, seed=42):\n",
        "    # CIFAR-10 mean/std (common)\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "    # Data augmentation ONLY for training\n",
        "    train_tfms = T.Compose([\n",
        "        # Random crop with padding for spatial augmentation\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        # Random horizontal flip for left-right invariance\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    # No augmentation for val/test\n",
        "    eval_tfms = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    # Load the official CIFAR10 train set twice with different transforms\n",
        "    full_train_for_train = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=True, download=True, transform=train_tfms\n",
        "    )\n",
        "    full_train_for_eval = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=True, download=True, transform=eval_tfms\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=False, download=True, transform=eval_tfms\n",
        "    )\n",
        "\n",
        "    # Split indices deterministically\n",
        "    n_total = len(full_train_for_train)  # 50,000\n",
        "    n_val = val_size\n",
        "    n_train = n_total - n_val\n",
        "\n",
        "    g = torch.Generator().manual_seed(seed)\n",
        "    train_subset, val_subset = random_split(full_train_for_train, [n_train, n_val], generator=g)\n",
        "\n",
        "    # IMPORTANT: val subset should use eval transforms (no augmentation)\n",
        "    # So we re-create val subset with the same indices but using full_train_for_eval\n",
        "    val_subset = torch.utils.data.Subset(full_train_for_eval, val_subset.indices)\n",
        "\n",
        "    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=num_workers, pin_memory=True)\n",
        "    valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=num_workers, pin_memory=True)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return trainloader, valloader, testloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_wGSEIq7IV"
      },
      "source": [
        "### Model Eval\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "34YYLjgkq-oz"
      },
      "outputs": [],
      "source": [
        "###Early Stopping Helper\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        # returns True if we should stop\n",
        "        improved = val_loss < (self.best_loss - self.min_delta)\n",
        "        if improved:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "\n",
        "\n",
        "###Eval\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRIOU9W_qg_M"
      },
      "source": [
        "## Train your model\n",
        "\n",
        "*   Please fill in the blanks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "chWtctKepEe8"
      },
      "outputs": [],
      "source": [
        "def train_with_early_stopping_adamw(\n",
        "    model,\n",
        "    trainloader,\n",
        "    valloader,\n",
        "    device,\n",
        "    epochs=50,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-2,\n",
        "    patience=3,\n",
        "    save_path=\"best_cifar10_simplecnn.pt\",\n",
        "):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # AdamW optimizer with learning rate and weight decay\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Cosine Annealing Learning Rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=patience, min_delta=0.0)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "        for x, y in trainloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss = loss_sum / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, valloader, device, criterion)\n",
        "\n",
        "        # Save best model by val loss\n",
        "        if val_loss < early_stopper.best_loss:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "            f\"train loss {train_loss:.4f} acc {train_acc*100:.2f}% | \"\n",
        "            f\"val loss {val_loss:.4f} acc {val_acc*100:.2f}% | \"\n",
        "            f\"best val acc {best_val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "        if early_stopper.step(val_loss):\n",
        "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
        "            break\n",
        "\n",
        "    # Load best weights\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEFoUoPzlJRj"
      },
      "source": [
        "### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pl7-OrzoqSwS"
      },
      "outputs": [],
      "source": [
        "###trainloader and testloader\n",
        "trainloader, valloader, testloader = get_cifar10_loaders(\n",
        "    batch_size=128,\n",
        "    num_workers=2,\n",
        "    val_size=5000,\n",
        "    seed=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qq9XCQYbqDOI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/billy/Documents/Hands-on-Deep-Learning/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 780x780 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# CIFAR-10 label names (0..9)\n",
        "classes = (\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
        "\n",
        "# If you used Normalize(mean,std) during training, use the same values here:\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "def unnormalize(img):\n",
        "    m = torch.tensor(mean).view(3, 1, 1)\n",
        "    s = torch.tensor(std).view(3, 1, 1)\n",
        "    return img * s + m\n",
        "\n",
        "def show_batch_from_loader(loader, n=16):\n",
        "    images, labels = next(iter(loader))   # images: (B,3,32,32), labels: (B,)\n",
        "    images, labels = images[:n].cpu(), labels[:n].cpu()\n",
        "\n",
        "    cols = int(np.ceil(np.sqrt(n)))\n",
        "    rows = int(np.ceil(n / cols))\n",
        "\n",
        "    plt.figure(figsize=(cols * 2.6, rows * 2.6))\n",
        "    for i in range(n):\n",
        "        img = unnormalize(images[i]).clamp(0, 1)          # back to [0,1]\n",
        "        img = img.permute(1, 2, 0).numpy()                # (H,W,C)\n",
        "\n",
        "        ax = plt.subplot(rows, cols, i + 1)\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(classes[int(labels[i])])\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_batch_from_loader(trainloader, n=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR9-obwclJRk"
      },
      "source": [
        "# Build Model 1\n",
        "#### Build a CNN model\n",
        "#### Architecture: from bottom (input) to top (output)\n",
        "\n",
        "  (input)\n",
        "\n",
        "\tâ€¢\tConv(3â†’32, 3Ã—3) + BN + ReLU     ###check nn.BatchNorm2d\n",
        "\tâ€¢\tConv(32â†’32, 3Ã—3) + BN + ReLU\n",
        "\tâ€¢\tMaxPool(2) + Dropout(0.25)\n",
        "\tâ€¢\tConv(32â†’64, 3Ã—3) + BN + ReLU\n",
        "\tâ€¢\tConv(64â†’64, 3Ã—3) + BN + ReLU\n",
        "\tâ€¢\tMaxPool(2) + Dropout(0.25)\n",
        "\tâ€¢\tConv(64â†’128, 3Ã—3) + BN + ReLU\n",
        "\tâ€¢\tGlobalAvgPool                   ###check nn.AdaptiveAvgPool2d\n",
        "\tâ€¢\tLinear(128â†’10)                  ###features to classes\n",
        "\n",
        "  (softmax + output)\n",
        "  \n",
        "\n",
        "#### We will use torch.optim.AdamW with lr=3e-4, weight_decay=1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zqaw9jVQlJRk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Conv(3->32, 3x3) + BN + ReLU\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        # Conv(32->32, 3x3) + BN + ReLU\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        # MaxPool(2) + Dropout(0.25)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.drop1 = nn.Dropout(0.25)\n",
        "\n",
        "        # Conv(32->64, 3x3) + BN + ReLU\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        # Conv(64->64, 3x3) + BN + ReLU\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        # MaxPool(2) + Dropout(0.25)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.drop2 = nn.Dropout(0.25)\n",
        "\n",
        "        # Conv(64->128, 3x3) + BN + ReLU\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # GlobalAvgPool\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Linear(128->10)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.drop1(self.pool1(x))\n",
        "\n",
        "        # Block 2\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.drop2(self.pool2(x))\n",
        "\n",
        "        # Block 3\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Classifier\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "model = SimpleCNN(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2XJTUXilJRk"
      },
      "source": [
        "### Inspect the model first: Summarize the model first to see we build the right one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BOcGCV1veuaD"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8sBIqJk_e27O"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "SimpleCNN                                [1, 10]                   --\n",
              "â”œâ”€Conv2d: 1-1                            [1, 32, 32, 32]           896\n",
              "â”œâ”€BatchNorm2d: 1-2                       [1, 32, 32, 32]           64\n",
              "â”œâ”€Conv2d: 1-3                            [1, 32, 32, 32]           9,248\n",
              "â”œâ”€BatchNorm2d: 1-4                       [1, 32, 32, 32]           64\n",
              "â”œâ”€MaxPool2d: 1-5                         [1, 32, 16, 16]           --\n",
              "â”œâ”€Dropout: 1-6                           [1, 32, 16, 16]           --\n",
              "â”œâ”€Conv2d: 1-7                            [1, 64, 16, 16]           18,496\n",
              "â”œâ”€BatchNorm2d: 1-8                       [1, 64, 16, 16]           128\n",
              "â”œâ”€Conv2d: 1-9                            [1, 64, 16, 16]           36,928\n",
              "â”œâ”€BatchNorm2d: 1-10                      [1, 64, 16, 16]           128\n",
              "â”œâ”€MaxPool2d: 1-11                        [1, 64, 8, 8]             --\n",
              "â”œâ”€Dropout: 1-12                          [1, 64, 8, 8]             --\n",
              "â”œâ”€Conv2d: 1-13                           [1, 128, 8, 8]            73,856\n",
              "â”œâ”€BatchNorm2d: 1-14                      [1, 128, 8, 8]            256\n",
              "â”œâ”€AdaptiveAvgPool2d: 1-15                [1, 128, 1, 1]            --\n",
              "â”œâ”€Linear: 1-16                           [1, 10]                   1,290\n",
              "==========================================================================================\n",
              "Total params: 141,354\n",
              "Trainable params: 141,354\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 29.30\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 1.70\n",
              "Params size (MB): 0.57\n",
              "Estimated Total Size (MB): 2.28\n",
              "=========================================================================================="
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model, input_size=(1, 3, 32, 32))  # CIFAR-10: 3x32x32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SimpleCNN Model Architecture Verification\n",
        "\n",
        "The model summary confirms our SimpleCNN is built correctly:\n",
        "\n",
        "**Architecture Flow:**\n",
        "1. **Input**: (1, 3, 32, 32) - CIFAR-10 RGB images\n",
        "2. **Block 1**: Conv(3->32) + BN + Conv(32->32) + BN + MaxPool -> (1, 32, 16, 16)\n",
        "3. **Block 2**: Conv(32->64) + BN + Conv(64->64) + BN + MaxPool -> (1, 64, 8, 8)\n",
        "4. **Block 3**: Conv(64->128) + BN -> (1, 128, 8, 8)\n",
        "5. **GAP**: AdaptiveAvgPool2d(1) -> (1, 128, 1, 1)\n",
        "6. **Output**: Linear(128->10) -> (1, 10)\n",
        "\n",
        "**Parameter Breakdown:**\n",
        "- Conv1 (3->32, 3x3): 3 x 32 x 3 x 3 + 32 = 896\n",
        "- Conv2 (32->32, 3x3): 32 x 32 x 3 x 3 + 32 = 9,248\n",
        "- Conv3 (32->64, 3x3): 32 x 64 x 3 x 3 + 64 = 18,496\n",
        "- Conv4 (64->64, 3x3): 64 x 64 x 3 x 3 + 64 = 36,928\n",
        "- Conv5 (64->128, 3x3): 64 x 128 x 3 x 3 + 128 = 73,856\n",
        "- FC (128->10): 128 x 10 + 10 = 1,290\n",
        "- BatchNorm layers: 64 + 64 + 128 + 128 + 256 = 640\n",
        "- **Total: 141,354 parameters**\n",
        "\n",
        "**Model Characteristics:**\n",
        "- Lightweight model (~0.57 MB)\n",
        "- Low computational cost (~29.30 MMACs)\n",
        "- Uses Global Average Pooling instead of FC layers to reduce parameters\n",
        "- Dropout (0.25) for regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RE-punEar-Ly"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/50 | train loss 1.6614 acc 39.72% | val loss 1.4311 acc 48.26% | best val acc 48.26%\n",
            "Epoch   2/50 | train loss 1.3167 acc 53.08% | val loss 1.2592 acc 53.92% | best val acc 53.92%\n",
            "Epoch   3/50 | train loss 1.1789 acc 57.95% | val loss 1.1473 acc 58.68% | best val acc 58.68%\n",
            "Epoch   4/50 | train loss 1.1000 acc 60.83% | val loss 1.1133 acc 60.54% | best val acc 60.54%\n",
            "Epoch   5/50 | train loss 1.0340 acc 63.33% | val loss 1.0205 acc 63.32% | best val acc 63.32%\n",
            "Epoch   6/50 | train loss 0.9862 acc 65.09% | val loss 0.9512 acc 66.34% | best val acc 66.34%\n",
            "Epoch   7/50 | train loss 0.9560 acc 66.26% | val loss 0.9318 acc 67.06% | best val acc 67.06%\n",
            "Epoch   8/50 | train loss 0.9176 acc 67.53% | val loss 0.9307 acc 67.80% | best val acc 67.80%\n",
            "Epoch   9/50 | train loss 0.8862 acc 68.63% | val loss 0.9092 acc 68.14% | best val acc 68.14%\n",
            "Epoch  10/50 | train loss 0.8656 acc 69.40% | val loss 0.8380 acc 70.30% | best val acc 70.30%\n",
            "Epoch  11/50 | train loss 0.8436 acc 70.07% | val loss 0.8839 acc 68.68% | best val acc 70.30%\n",
            "Epoch  12/50 | train loss 0.8193 acc 71.25% | val loss 0.8940 acc 69.86% | best val acc 70.30%\n",
            "Epoch  13/50 | train loss 0.8030 acc 71.82% | val loss 0.7985 acc 71.74% | best val acc 71.74%\n",
            "Epoch  14/50 | train loss 0.7805 acc 72.46% | val loss 0.8447 acc 71.82% | best val acc 71.82%\n",
            "Epoch  15/50 | train loss 0.7692 acc 73.07% | val loss 0.8405 acc 71.30% | best val acc 71.82%\n",
            "Epoch  16/50 | train loss 0.7558 acc 73.50% | val loss 0.8123 acc 72.36% | best val acc 72.36%\n",
            "Early stopping triggered at epoch 16.\n"
          ]
        }
      ],
      "source": [
        "# Run the model\n",
        "# Use patience = 3\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
        "model = model.to(device)  # Ensure model is on the correct device\n",
        "\n",
        "\n",
        "model_train = train_with_early_stopping_adamw(\n",
        "    model,\n",
        "    trainloader,\n",
        "    valloader,\n",
        "    device,\n",
        "    epochs=50,\n",
        "    lr=3e-4,   ### 3e-4 usually works well\n",
        "    weight_decay=1e-2,  ###1e-2 usually works well\n",
        "    patience=3,\n",
        "    save_path=\"best_cifar10_simplecnn_adamw.pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_zBG3TylJRl"
      },
      "source": [
        "### Final test evaluation\n",
        "### What is the test_loss and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5JWNB1qNGMZy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FINAL TEST | loss 0.8020 acc 72.12%\n"
          ]
        }
      ],
      "source": [
        "# Final test evaluation for SimpleCNN (Model 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc = evaluate(model_train, testloader, device, criterion)\n",
        "\n",
        "print(f\"FINAL TEST | loss {test_loss:.4f} acc {test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SimpleCNN Training Analysis\n",
        "\n",
        "**Training Dynamics:**\n",
        "- **Epochs trained**: 16 (early stopping triggered)\n",
        "- **Best validation accuracy**: 72.36% (epoch 16)\n",
        "- **Final test accuracy**: 72.12%\n",
        "- **Final test loss**: 0.8020\n",
        "- **Generalization gap**: 73.5% (train) - 72.12% (test) = **1.38%**\n",
        "\n",
        "**Learning Progression:**\n",
        "| Phase | Epochs | Train Acc | Val Acc | Observations |\n",
        "|-------|--------|-----------|---------|--------------|\n",
        "| Early | 1-5 | 39.7% -> 63.3% | 48.3% -> 63.3% | Rapid learning, losses decreasing steadily |\n",
        "| Middle | 6-10 | 65.1% -> 69.4% | 66.3% -> 70.3% | Slower improvement, model learning finer features |\n",
        "| Late | 11-16 | 70.1% -> 73.5% | 68.7% -> 72.4% | Diminishing returns, early stopping triggered |\n",
        "\n",
        "**Key Observations:**\n",
        "1. **Convergence Speed**: Model converged relatively quickly (16 epochs) due to limited capacity\n",
        "2. **Best Generalization**: Smallest gap (1.38%) between train and test accuracy among all models - the model does not overfit\n",
        "3. **Early Stopping Effectiveness**: Patience=3 prevented overfitting; validation accuracy plateaued around 72%\n",
        "4. **Loss Behavior**: Training loss decreased from 1.66 to 0.76, validation loss from 1.43 to 0.81\n",
        "\n",
        "**Limitations:**\n",
        "- Model capacity (141K params) limits ability to learn complex patterns\n",
        "- **Lowest test accuracy** (72.12%) despite best generalization - underfitting due to limited capacity\n",
        "- Would benefit from deeper architecture or more channels for higher accuracy\n",
        "\n",
        "**Key Distinction:**\n",
        "- **Test Accuracy** (absolute performance): 72.12% - lowest among all models\n",
        "- **Generalization Gap** (train-test difference): 1.38% - best among all models (least overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7w8_wG5K6T"
      },
      "source": [
        "# Question: What if we do not have the data augmentation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWEnrxEV5KtQ"
      },
      "source": [
        "## Summary: Impact of Data Augmentation\n",
        "\n",
        "**Without data augmentation:**\n",
        "- The model would likely overfit to the training data more quickly\n",
        "- Training accuracy would be higher, but validation/test accuracy would be lower\n",
        "- The gap between training and validation accuracy would be larger (overfitting indicator)\n",
        "- Early stopping would trigger earlier due to validation loss increasing\n",
        "\n",
        "**With data augmentation (RandomCrop + RandomHorizontalFlip):**\n",
        "- The model sees more diverse variations of the training images\n",
        "- RandomCrop with padding=4 creates spatial translations, making the model robust to object position\n",
        "- RandomHorizontalFlip doubles the effective dataset size and teaches left-right invariance\n",
        "- Better generalization to unseen test data\n",
        "- Reduced overfitting, smaller train-val accuracy gap\n",
        "\n",
        "**Key takeaway:** Data augmentation is a form of regularization that artificially expands the training set, helping the model learn more robust features that generalize better to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbVp3OCxyLIf"
      },
      "source": [
        "# Build Model 2: VGG-style model and repeat the above analysis for model 1\n",
        "### Architecture\n",
        "\n",
        "    Input: (N, 3, 32, 32)\n",
        "\n",
        "    Block 1 (channels 64)\n",
        "      1.\tConv3Ã—3, 3â†’64, pad=1 â†’ (N, 64, 32, 32)\n",
        "      2.\tBN(64) + ReLU\n",
        "      3.\tConv3Ã—3, 64â†’64, pad=1 â†’ (N, 64, 32, 32)\n",
        "      4.\tBN(64) + ReLU\n",
        "      5.\tMaxPool2Ã—2 â†’ (N, 64, 16, 16)\n",
        "      6.\tDropout(p=0.25)\n",
        "\n",
        "    Block 2 (channels 128)\n",
        "      7.\tConv3Ã—3, 64â†’128, pad=1 â†’ (N, 128, 16, 16)\n",
        "      8.\tBN(128) + ReLU\n",
        "      9.\tConv3Ã—3, 128â†’128, pad=1 â†’ (N, 128, 16, 16)\n",
        "      10.\tBN(128) + ReLU\n",
        "      11.\tMaxPool2Ã—2 â†’ (N, 128, 8, 8)\n",
        "      12.\tDropout(p=0.25)\n",
        "\n",
        "    Block 3 (channels 256)\n",
        "      13.\tConv3Ã—3, 128â†’256, pad=1 â†’ (N, 256, 8, 8)\n",
        "      14.\tBN(256) + ReLU\n",
        "      15.\tConv3Ã—3, 256â†’256, pad=1 â†’ (N, 256, 8, 8)\n",
        "      16.\tBN(256) + ReLU\n",
        "      17.\tMaxPool2Ã—2 â†’ (N, 256, 4, 4)\n",
        "      18.\tDropout(p=0.25)\n",
        "\n",
        "    Head\n",
        "      19.\tAdaptiveAvgPool2d(1) â†’ (N, 256, 1, 1)\n",
        "      20.\tFlatten â†’ (N, 256)\n",
        "      21.\tLinear 256â†’10 â†’ (N, 10)\n",
        "\n",
        "    Softmax + Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Pr0_93eyyOGW"
      },
      "outputs": [],
      "source": [
        "class VGGSmallCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG-style network for CIFAR-10:\n",
        "      (64x2) -> pool -> (128x2) -> pool -> (256x2) -> pool -> GAP -> FC\n",
        "    Input:  (N,3,32,32)\n",
        "    Output: (N,10)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, dropout=0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        # Block 1 (channels 64)\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),      # (N, 64, 32, 32)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),     # (N, 64, 32, 32)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                                  # (N, 64, 16, 16)\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Block 2 (channels 128)\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),    # (N, 128, 16, 16)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),   # (N, 128, 16, 16)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                                  # (N, 128, 8, 8)\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Block 3 (channels 256)\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),   # (N, 256, 8, 8)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),   # (N, 256, 8, 8)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                                  # (N, 256, 4, 4)\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),                          # (N, 256, 1, 1)\n",
        "            nn.Flatten(),                                     # (N, 256)\n",
        "            nn.Linear(256, num_classes),                      # (N, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CfMmQCFtyOun"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGGSmallCIFAR                            [1, 10]                   --\n",
              "â”œâ”€Sequential: 1-1                        [1, 64, 16, 16]           --\n",
              "â”‚    â””â”€Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
              "â”‚    â””â”€BatchNorm2d: 2-2                  [1, 64, 32, 32]           128\n",
              "â”‚    â””â”€ReLU: 2-3                         [1, 64, 32, 32]           --\n",
              "â”‚    â””â”€Conv2d: 2-4                       [1, 64, 32, 32]           36,928\n",
              "â”‚    â””â”€BatchNorm2d: 2-5                  [1, 64, 32, 32]           128\n",
              "â”‚    â””â”€ReLU: 2-6                         [1, 64, 32, 32]           --\n",
              "â”‚    â””â”€MaxPool2d: 2-7                    [1, 64, 16, 16]           --\n",
              "â”‚    â””â”€Dropout: 2-8                      [1, 64, 16, 16]           --\n",
              "â”œâ”€Sequential: 1-2                        [1, 128, 8, 8]            --\n",
              "â”‚    â””â”€Conv2d: 2-9                       [1, 128, 16, 16]          73,856\n",
              "â”‚    â””â”€BatchNorm2d: 2-10                 [1, 128, 16, 16]          256\n",
              "â”‚    â””â”€ReLU: 2-11                        [1, 128, 16, 16]          --\n",
              "â”‚    â””â”€Conv2d: 2-12                      [1, 128, 16, 16]          147,584\n",
              "â”‚    â””â”€BatchNorm2d: 2-13                 [1, 128, 16, 16]          256\n",
              "â”‚    â””â”€ReLU: 2-14                        [1, 128, 16, 16]          --\n",
              "â”‚    â””â”€MaxPool2d: 2-15                   [1, 128, 8, 8]            --\n",
              "â”‚    â””â”€Dropout: 2-16                     [1, 128, 8, 8]            --\n",
              "â”œâ”€Sequential: 1-3                        [1, 256, 4, 4]            --\n",
              "â”‚    â””â”€Conv2d: 2-17                      [1, 256, 8, 8]            295,168\n",
              "â”‚    â””â”€BatchNorm2d: 2-18                 [1, 256, 8, 8]            512\n",
              "â”‚    â””â”€ReLU: 2-19                        [1, 256, 8, 8]            --\n",
              "â”‚    â””â”€Conv2d: 2-20                      [1, 256, 8, 8]            590,080\n",
              "â”‚    â””â”€BatchNorm2d: 2-21                 [1, 256, 8, 8]            512\n",
              "â”‚    â””â”€ReLU: 2-22                        [1, 256, 8, 8]            --\n",
              "â”‚    â””â”€MaxPool2d: 2-23                   [1, 256, 4, 4]            --\n",
              "â”‚    â””â”€Dropout: 2-24                     [1, 256, 4, 4]            --\n",
              "â”œâ”€Sequential: 1-4                        [1, 10]                   --\n",
              "â”‚    â””â”€AdaptiveAvgPool2d: 2-25           [1, 256, 1, 1]            --\n",
              "â”‚    â””â”€Flatten: 2-26                     [1, 256]                  --\n",
              "â”‚    â””â”€Linear: 2-27                      [1, 10]                   2,570\n",
              "==========================================================================================\n",
              "Total params: 1,149,770\n",
              "Trainable params: 1,149,770\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 153.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 3.67\n",
              "Params size (MB): 4.60\n",
              "Estimated Total Size (MB): 8.28\n",
              "=========================================================================================="
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "model2 = VGGSmallCIFAR(num_classes=10).to(device)\n",
        "\n",
        "summary(model2, input_size=(1, 3, 32, 32))\n",
        "\n",
        "# x = torch.randn(4, 3, 32, 32).to(device)\n",
        "# y = model(x)\n",
        "# print(\"Output shape:\", y.shape)  # should be torch.Size([4, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aDcnrE_N9Yos"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/50 | train loss 1.4334 acc 47.75% | val loss 1.3901 acc 51.96% | best val acc 51.96%\n",
            "Epoch   2/50 | train loss 1.0502 acc 62.53% | val loss 0.9603 acc 65.32% | best val acc 65.32%\n",
            "Epoch   3/50 | train loss 0.8997 acc 68.13% | val loss 0.8904 acc 68.28% | best val acc 68.28%\n",
            "Epoch   4/50 | train loss 0.7941 acc 72.34% | val loss 0.7974 acc 71.42% | best val acc 71.42%\n",
            "Epoch   5/50 | train loss 0.7192 acc 75.24% | val loss 0.6490 acc 76.96% | best val acc 76.96%\n",
            "Epoch   6/50 | train loss 0.6644 acc 77.00% | val loss 0.6342 acc 77.94% | best val acc 77.94%\n",
            "Epoch   7/50 | train loss 0.6207 acc 78.62% | val loss 0.6297 acc 78.20% | best val acc 78.20%\n",
            "Epoch   8/50 | train loss 0.5824 acc 79.80% | val loss 0.5716 acc 79.96% | best val acc 79.96%\n",
            "Epoch   9/50 | train loss 0.5538 acc 80.76% | val loss 0.5363 acc 81.82% | best val acc 81.82%\n",
            "Epoch  10/50 | train loss 0.5276 acc 81.64% | val loss 0.5732 acc 80.18% | best val acc 81.82%\n",
            "Epoch  11/50 | train loss 0.5089 acc 82.50% | val loss 0.5366 acc 81.78% | best val acc 81.82%\n",
            "Epoch  12/50 | train loss 0.4835 acc 83.37% | val loss 0.5127 acc 82.64% | best val acc 82.64%\n",
            "Epoch  13/50 | train loss 0.4620 acc 83.88% | val loss 0.4610 acc 84.22% | best val acc 84.22%\n",
            "Epoch  14/50 | train loss 0.4472 acc 84.61% | val loss 0.4324 acc 85.06% | best val acc 85.06%\n",
            "Epoch  15/50 | train loss 0.4323 acc 85.00% | val loss 0.4968 acc 83.44% | best val acc 85.06%\n",
            "Epoch  16/50 | train loss 0.4169 acc 85.70% | val loss 0.4515 acc 84.58% | best val acc 85.06%\n",
            "Epoch  17/50 | train loss 0.4051 acc 86.08% | val loss 0.4254 acc 85.62% | best val acc 85.62%\n",
            "Epoch  18/50 | train loss 0.3928 acc 86.32% | val loss 0.4278 acc 85.08% | best val acc 85.62%\n",
            "Epoch  19/50 | train loss 0.3791 acc 86.92% | val loss 0.4328 acc 85.60% | best val acc 85.62%\n",
            "Epoch  20/50 | train loss 0.3691 acc 87.22% | val loss 0.4153 acc 85.78% | best val acc 85.78%\n",
            "Epoch  21/50 | train loss 0.3563 acc 87.71% | val loss 0.4067 acc 85.72% | best val acc 85.78%\n",
            "Epoch  22/50 | train loss 0.3469 acc 88.08% | val loss 0.3952 acc 86.96% | best val acc 86.96%\n",
            "Epoch  23/50 | train loss 0.3342 acc 88.40% | val loss 0.3811 acc 86.30% | best val acc 86.96%\n",
            "Epoch  24/50 | train loss 0.3251 acc 88.68% | val loss 0.3557 acc 87.98% | best val acc 87.98%\n",
            "Epoch  25/50 | train loss 0.3156 acc 89.10% | val loss 0.3439 acc 88.14% | best val acc 88.14%\n",
            "Epoch  26/50 | train loss 0.3069 acc 89.25% | val loss 0.3698 acc 87.60% | best val acc 88.14%\n",
            "Epoch  27/50 | train loss 0.2979 acc 89.69% | val loss 0.3488 acc 87.98% | best val acc 88.14%\n",
            "Epoch  28/50 | train loss 0.2936 acc 89.74% | val loss 0.3489 acc 88.30% | best val acc 88.30%\n",
            "Early stopping triggered at epoch 28.\n",
            "FINAL TEST | loss 0.3657 acc 87.92%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
        "model2 = model2.to(device)  # Ensure model is on the correct device\n",
        "\n",
        "model2_train = train_with_early_stopping_adamw(\n",
        "    model2,\n",
        "    trainloader,\n",
        "    valloader,\n",
        "    device,\n",
        "    epochs=50,\n",
        "    lr=3e-4,              # lr = 3e-4\n",
        "    weight_decay=1e-2,    # weight_decay = 1e-2\n",
        "    patience=3,           # patience = 3\n",
        "    save_path=\"best_vggsmall_cifar10_adamw.pt\"\n",
        ")\n",
        "\n",
        "# Final test verification for VGGSmallCIFAR (Model 2)\n",
        "test_loss, test_acc = evaluate(model2_train, testloader, device, nn.CrossEntropyLoss())\n",
        "print(f\"FINAL TEST | loss {test_loss:.4f} acc {test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGGSmallCIFAR Training Analysis\n",
        "\n",
        "**Training Dynamics:**\n",
        "- **Epochs trained**: 28 (early stopping triggered)\n",
        "- **Best validation accuracy**: 88.30% (epoch 28)\n",
        "- **Final test accuracy**: 87.92%\n",
        "- **Final test loss**: 0.3657\n",
        "- **Generalization gap**: 89.74% (train) - 87.92% (test) = **1.82%**\n",
        "\n",
        "**Learning Progression:**\n",
        "| Phase | Epochs | Train Acc | Val Acc | Observations |\n",
        "|-------|--------|-----------|---------|--------------|\n",
        "| Early | 1-7 | 47.8% -> 78.6% | 52.0% -> 78.2% | Very rapid learning due to higher capacity |\n",
        "| Middle | 8-17 | 79.8% -> 86.1% | 80.0% -> 85.6% | Steady improvement, learning complex patterns |\n",
        "| Late | 18-28 | 86.3% -> 89.7% | 85.1% -> 88.3% | Fine-tuning, small but consistent gains |\n",
        "\n",
        "**Key Observations:**\n",
        "1. **Highest Test Accuracy**: 87.92% - best absolute performance among all models\n",
        "2. **Good Generalization**: Gap of 1.82% between train and test accuracy (second best after SimpleCNN)\n",
        "3. **Higher Capacity Benefits**: 8x more parameters than SimpleCNN enables learning more complex features\n",
        "4. **Longer Training**: Trained 28 epochs vs 16 for SimpleCNN - more capacity requires more training\n",
        "5. **Loss Convergence**: Training loss dropped from 1.43 to 0.29, validation from 1.39 to 0.35\n",
        "\n",
        "**Comparison with SimpleCNN:**\n",
        "| Metric | SimpleCNN | VGGSmallCIFAR | Difference |\n",
        "|--------|-----------|---------------|------------|\n",
        "| Test Accuracy | 72.12% | 87.92% | +15.80% (VGG better) |\n",
        "| Generalization Gap | 1.38% | 1.82% | +0.44% (SimpleCNN better) |\n",
        "| Parameters | 141K | 1.15M | 8.1x more |\n",
        "\n",
        "**Insights:**\n",
        "- VGG achieves **best test accuracy** due to higher model capacity\n",
        "- Dropout (0.25) after each block keeps generalization gap small despite large capacity\n",
        "- The model balances high capacity with good regularization\n",
        "\n",
        "**Key Distinction:**\n",
        "- **Test Accuracy** (absolute performance): 87.92% - highest among all models\n",
        "- **Generalization Gap** (train-test difference): 1.82% - good but not the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VGGSmallCIFAR Model Architecture Verification\n",
        "\n",
        "The model summary confirms our VGG-style network is built correctly:\n",
        "\n",
        "**Architecture Flow:**\n",
        "1. **Input**: (1, 3, 32, 32) - CIFAR-10 RGB images\n",
        "2. **Block 1**: [Conv(3->64) + BN + ReLU] x 2 + MaxPool + Dropout -> (1, 64, 16, 16)\n",
        "3. **Block 2**: [Conv(64->128) + BN + ReLU] x 2 + MaxPool + Dropout -> (1, 128, 8, 8)\n",
        "4. **Block 3**: [Conv(128->256) + BN + ReLU] x 2 + MaxPool + Dropout -> (1, 256, 4, 4)\n",
        "5. **Head**: AdaptiveAvgPool2d(1) + Flatten + Linear(256->10) -> (1, 10)\n",
        "\n",
        "**Parameter Breakdown by Block:**\n",
        "- **Block 1** (64 channels): 1,792 + 128 + 36,928 + 128 = 38,976\n",
        "- **Block 2** (128 channels): 73,856 + 256 + 147,584 + 256 = 221,952\n",
        "- **Block 3** (256 channels): 295,168 + 512 + 590,080 + 512 = 886,272\n",
        "- **Head**: 2,570\n",
        "- **Total: 1,149,770 parameters (~8x more than SimpleCNN)**\n",
        "\n",
        "**Comparison with SimpleCNN:**\n",
        "| Metric | SimpleCNN | VGGSmallCIFAR |\n",
        "|--------|-----------|---------------|\n",
        "| Parameters | 141,354 | 1,149,770 |\n",
        "| Model Size | 0.57 MB | 4.60 MB |\n",
        "| Mult-Adds | 29.30 M | 153.00 M |\n",
        "| Memory | 2.28 MB | 8.28 MB |\n",
        "\n",
        "**Key Observations:**\n",
        "- VGG uses wider layers (64->128->256) compared to SimpleCNN (32->64->128)\n",
        "- Consistent VGG-style pattern: double convolutions before each pooling\n",
        "- Higher capacity model should achieve better accuracy but requires more compute\n",
        "- Dropout after each block helps prevent overfitting despite larger capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZrJpFKzFZ3"
      },
      "source": [
        "# Build Model 3: Resnet\n",
        "#### For Resnet, please refer to the paper: https://arxiv.org/abs/1512.03385\n",
        "\n",
        "#### Architecture\n",
        "    Overall structure (CIFAR ResNet)\n",
        "\n",
        "    Input: (N, 3, 32, 32)\n",
        "\n",
        "    Stem\n",
        "      1.\tConv 3Ã—3, 3 â†’ 16, stride 1, pad 1\n",
        "      2.\tBatchNorm(16)\n",
        "      3.\tReLU\n",
        "    Output: (N, 16, 32, 32)\n",
        "\n",
        "    â¸»\n",
        "\n",
        "    Residual stages (BasicBlock)\n",
        "\n",
        "    Each BasicBlock is:\n",
        "      â€¢\tConv 3Ã—3 (stride = s), BN, ReLU\n",
        "      â€¢\tConv 3Ã—3 (stride = 1), BN\n",
        "      â€¢\tAdd shortcut (identity if shape matches, else 1Ã—1 conv + BN)\n",
        "      â€¢\tReLU\n",
        "\n",
        "    Stage 1: width 16, spatial 32Ã—32\n",
        "      â€¢\t3 BasicBlocks (for ResNet-20)\n",
        "      â€¢\tFirst block stride=1 (no downsampling)\n",
        "    Output stays: (N, 16, 32, 32)\n",
        "\n",
        "    Stage 2: width 32, spatial 16Ã—16\n",
        "      â€¢\t3 BasicBlocks\n",
        "      â€¢\tFirst block uses stride=2 â†’ downsamples 32Ã—32 â†’ 16Ã—16\n",
        "      â€¢\tShortcut uses 1Ã—1 conv stride=2 (because channels/spatial change)\n",
        "    Output: (N, 32, 16, 16)\n",
        "\n",
        "    Stage 3: width 64, spatial 8Ã—8\n",
        "      â€¢\t3 BasicBlocks\n",
        "      â€¢\tFirst block uses stride=2 â†’ downsamples 16Ã—16 â†’ 8Ã—8\n",
        "      â€¢\tShortcut uses 1Ã—1 conv stride=2\n",
        "    Output: (N, 64, 8, 8)\n",
        "\n",
        "    â¸»\n",
        "\n",
        "    Head\n",
        "      â€¢\tGlobal Average Pool (AdaptiveAvgPool2d(1)): (N, 64, 1, 1)\n",
        "      â€¢\tFlatten â†’ (N, 64)\n",
        "      â€¢\tLinear 64 â†’ 10 â†’ (N, 10)\n",
        "\n",
        "\n",
        "#### Please repeat your analysis and then summarize your findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6GpStLXxzI_s"
      },
      "outputs": [],
      "source": [
        "###THe BasicBlock is complete\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes: int, planes: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Projection shortcut if shape changes (stride or channels)\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + self.shortcut(x)\n",
        "        out = F.relu(out, inplace=True)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "####Please fill in the blanks in this class\n",
        "class ResNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR-style ResNet:\n",
        "      stem: 3x3 conv, 16ch\n",
        "      stage1: 16ch blocks, stride 1\n",
        "      stage2: 32ch blocks, stride 2\n",
        "      stage3: 64ch blocks, stride 2\n",
        "      GAP + FC\n",
        "    \"\"\"\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        # Stem: Conv 3x3, 3 -> 16, stride 1, pad 1\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Residual stages\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)   # 16ch, 32x32\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)   # 32ch, 16x16\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)   # 64ch, 8x8\n",
        "\n",
        "        # Head: Global Average Pool + Linear\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "    ####helper: _make_layer for layers 1, 2, 3\n",
        "    def _make_layer(self, block, planes, n_blocks, stride):\n",
        "        strides = [stride] + [1] * (n_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride=s))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stem\n",
        "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "\n",
        "        # Residual stages\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        # Head\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "####main restnet\n",
        "def resnet20_cifar(num_classes=10):\n",
        "    # 6n+2 layers, n=3 -> 20\n",
        "    return ResNetCIFAR(BasicBlock, [3, 3, 3], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9EUB1avB6BC1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNetCIFAR                              [1, 10]                   --\n",
              "â”œâ”€Conv2d: 1-1                            [1, 16, 32, 32]           432\n",
              "â”œâ”€BatchNorm2d: 1-2                       [1, 16, 32, 32]           32\n",
              "â”œâ”€Sequential: 1-3                        [1, 16, 32, 32]           --\n",
              "â”‚    â””â”€BasicBlock: 2-1                   [1, 16, 32, 32]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-3                  [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-4             [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Identity: 3-5                [1, 16, 32, 32]           --\n",
              "â”‚    â””â”€BasicBlock: 2-2                   [1, 16, 32, 32]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-6                  [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-7             [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-8                  [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-9             [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Identity: 3-10               [1, 16, 32, 32]           --\n",
              "â”‚    â””â”€BasicBlock: 2-3                   [1, 16, 32, 32]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-11                 [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-12            [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 16, 32, 32]           2,304\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-14            [1, 16, 32, 32]           32\n",
              "â”‚    â”‚    â””â”€Identity: 3-15               [1, 16, 32, 32]           --\n",
              "â”œâ”€Sequential: 1-4                        [1, 32, 16, 16]           --\n",
              "â”‚    â””â”€BasicBlock: 2-4                   [1, 32, 16, 16]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-16                 [1, 32, 16, 16]           4,608\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-17            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-18                 [1, 32, 16, 16]           9,216\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-19            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Sequential: 3-20             [1, 32, 16, 16]           576\n",
              "â”‚    â””â”€BasicBlock: 2-5                   [1, 32, 16, 16]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-21                 [1, 32, 16, 16]           9,216\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-22            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-23                 [1, 32, 16, 16]           9,216\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-24            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Identity: 3-25               [1, 32, 16, 16]           --\n",
              "â”‚    â””â”€BasicBlock: 2-6                   [1, 32, 16, 16]           --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-26                 [1, 32, 16, 16]           9,216\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-27            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-28                 [1, 32, 16, 16]           9,216\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-29            [1, 32, 16, 16]           64\n",
              "â”‚    â”‚    â””â”€Identity: 3-30               [1, 32, 16, 16]           --\n",
              "â”œâ”€Sequential: 1-5                        [1, 64, 8, 8]             --\n",
              "â”‚    â””â”€BasicBlock: 2-7                   [1, 64, 8, 8]             --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-31                 [1, 64, 8, 8]             18,432\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-32            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-33                 [1, 64, 8, 8]             36,864\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-34            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Sequential: 3-35             [1, 64, 8, 8]             2,176\n",
              "â”‚    â””â”€BasicBlock: 2-8                   [1, 64, 8, 8]             --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-36                 [1, 64, 8, 8]             36,864\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-37            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-38                 [1, 64, 8, 8]             36,864\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-39            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Identity: 3-40               [1, 64, 8, 8]             --\n",
              "â”‚    â””â”€BasicBlock: 2-9                   [1, 64, 8, 8]             --\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-41                 [1, 64, 8, 8]             36,864\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-42            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Conv2d: 3-43                 [1, 64, 8, 8]             36,864\n",
              "â”‚    â”‚    â””â”€BatchNorm2d: 3-44            [1, 64, 8, 8]             128\n",
              "â”‚    â”‚    â””â”€Identity: 3-45               [1, 64, 8, 8]             --\n",
              "â”œâ”€AdaptiveAvgPool2d: 1-6                 [1, 64, 1, 1]             --\n",
              "â”œâ”€Linear: 1-7                            [1, 10]                   650\n",
              "==========================================================================================\n",
              "Total params: 272,474\n",
              "Trainable params: 272,474\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 40.81\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 3.21\n",
              "Params size (MB): 1.09\n",
              "Estimated Total Size (MB): 4.31\n",
              "=========================================================================================="
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "model3 = resnet20_cifar(num_classes=10).to(device)\n",
        "\n",
        "summary(model3, input_size=(1, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet-20 Model Architecture Verification\n",
        "\n",
        "The model summary confirms our ResNet-20 is built correctly following the CIFAR-style architecture:\n",
        "\n",
        "**Architecture Flow:**\n",
        "1. **Input**: (1, 3, 32, 32) - CIFAR-10 RGB images\n",
        "2. **Stem**: Conv(3->16, 3x3) + BN -> (1, 16, 32, 32)\n",
        "3. **Stage 1**: 3 BasicBlocks, 16 channels, stride 1 -> (1, 16, 32, 32)\n",
        "4. **Stage 2**: 3 BasicBlocks, 32 channels, stride 2 -> (1, 32, 16, 16)\n",
        "5. **Stage 3**: 3 BasicBlocks, 64 channels, stride 2 -> (1, 64, 8, 8)\n",
        "6. **Head**: AdaptiveAvgPool2d(1) + Linear(64->10) -> (1, 10)\n",
        "\n",
        "**Layer Count Verification (6n+2 = 20):**\n",
        "- Stem: 1 conv layer\n",
        "- Stage 1: 3 blocks x 2 conv = 6 layers\n",
        "- Stage 2: 3 blocks x 2 conv = 6 layers\n",
        "- Stage 3: 3 blocks x 2 conv = 6 layers\n",
        "- FC: 1 layer\n",
        "- **Total: 1 + 6 + 6 + 6 + 1 = 20 layers**\n",
        "\n",
        "**Parameter Breakdown by Stage:**\n",
        "- **Stem**: 432 + 32 = 464\n",
        "- **Stage 1** (16ch, 3 blocks): 3 x (2,304 + 32 + 2,304 + 32) = 14,016\n",
        "- **Stage 2** (32ch, 3 blocks): 14,528 + 576 (shortcut) + 2 x 18,560 = 52,224\n",
        "- **Stage 3** (64ch, 3 blocks): 57,728 + 2,176 (shortcut) + 2 x 74,112 = 208,128\n",
        "- **FC**: 650\n",
        "- **Total: 272,474 parameters**\n",
        "\n",
        "**Comparison with Other Models:**\n",
        "| Metric | SimpleCNN | VGGSmallCIFAR | ResNet-20 |\n",
        "|--------|-----------|---------------|-----------|\n",
        "| Parameters | 141,354 | 1,149,770 | 272,474 |\n",
        "| Model Size | 0.57 MB | 4.60 MB | 1.09 MB |\n",
        "| Mult-Adds | 29.30 M | 153.00 M | 40.81 M |\n",
        "| Memory | 2.28 MB | 8.28 MB | 4.31 MB |\n",
        "\n",
        "**Key Observations:**\n",
        "- ResNet-20 has ~2x parameters of SimpleCNN but ~4x fewer than VGG\n",
        "- Skip connections (Identity/Sequential shortcuts) enable deeper training\n",
        "- Projection shortcuts (1x1 conv) used when dimensions change (stage transitions)\n",
        "- Efficient design: more depth with fewer parameters than VGG\n",
        "- Follows original ResNet paper design for CIFAR (16->32->64 channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8JhrW9ey-7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/50 | train loss 1.5753 acc 41.29% | val loss 1.4047 acc 49.96% | best val acc 49.96%\n",
            "Epoch   2/50 | train loss 1.2176 acc 56.08% | val loss 1.1962 acc 57.02% | best val acc 57.02%\n",
            "Epoch   3/50 | train loss 1.0339 acc 63.12% | val loss 1.0071 acc 64.46% | best val acc 64.46%\n",
            "Epoch   4/50 | train loss 0.9143 acc 67.63% | val loss 0.9489 acc 66.48% | best val acc 66.48%\n",
            "Epoch   5/50 | train loss 0.8307 acc 70.97% | val loss 0.8610 acc 70.12% | best val acc 70.12%\n",
            "Epoch   6/50 | train loss 0.7628 acc 73.45% | val loss 0.8675 acc 70.64% | best val acc 70.64%\n",
            "Epoch   7/50 | train loss 0.7075 acc 75.22% | val loss 0.7786 acc 73.52% | best val acc 73.52%\n",
            "Epoch   8/50 | train loss 0.6661 acc 77.09% | val loss 0.6948 acc 75.42% | best val acc 75.42%\n",
            "Epoch   9/50 | train loss 0.6310 acc 78.28% | val loss 0.6895 acc 76.22% | best val acc 76.22%\n",
            "Epoch  10/50 | train loss 0.5985 acc 79.30% | val loss 0.6492 acc 77.90% | best val acc 77.90%\n",
            "Epoch  11/50 | train loss 0.5759 acc 80.15% | val loss 0.6475 acc 77.52% | best val acc 77.90%\n",
            "Epoch  12/50 | train loss 0.5493 acc 80.94% | val loss 0.7935 acc 74.74% | best val acc 77.90%\n",
            "Epoch  13/50 | train loss 0.5297 acc 81.62% | val loss 0.6524 acc 77.68% | best val acc 77.90%\n",
            "Epoch  14/50 | train loss 0.5110 acc 82.25% | val loss 0.6302 acc 77.92% | best val acc 77.92%\n",
            "Epoch  15/50 | train loss 0.4886 acc 83.18% | val loss 0.5320 acc 81.48% | best val acc 81.48%\n",
            "Epoch  16/50 | train loss 0.4769 acc 83.53% | val loss 0.5796 acc 79.78% | best val acc 81.48%\n",
            "Epoch  17/50 | train loss 0.4601 acc 84.26% | val loss 0.5416 acc 81.82% | best val acc 81.82%\n",
            "Epoch  18/50 | train loss 0.4470 acc 84.30% | val loss 0.5175 acc 82.74% | best val acc 82.74%\n",
            "Epoch  19/50 | train loss 0.4349 acc 84.92% | val loss 0.5093 acc 82.64% | best val acc 82.74%\n",
            "Epoch  20/50 | train loss 0.4210 acc 85.32% | val loss 0.4806 acc 83.22% | best val acc 83.22%\n",
            "Epoch  21/50 | train loss 0.4066 acc 86.09% | val loss 0.5005 acc 83.08% | best val acc 83.22%\n",
            "Epoch  22/50 | train loss 0.3990 acc 86.28% | val loss 0.4699 acc 84.06% | best val acc 84.06%\n",
            "Epoch  23/50 | train loss 0.3819 acc 86.72% | val loss 0.4920 acc 83.06% | best val acc 84.06%\n",
            "Epoch  24/50 | train loss 0.3741 acc 87.13% | val loss 0.4694 acc 84.26% | best val acc 84.26%\n",
            "Epoch  25/50 | train loss 0.3618 acc 87.47% | val loss 0.4860 acc 83.52% | best val acc 84.26%\n",
            "Epoch  26/50 | train loss 0.3541 acc 87.79% | val loss 0.4762 acc 83.90% | best val acc 84.26%\n",
            "Epoch  27/50 | train loss 0.3447 acc 88.02% | val loss 0.4636 acc 84.68% | best val acc 84.68%\n",
            "Epoch  28/50 | train loss 0.3343 acc 88.34% | val loss 0.4544 acc 84.80% | best val acc 84.80%\n",
            "Epoch  29/50 | train loss 0.3255 acc 88.72% | val loss 0.4565 acc 84.78% | best val acc 84.80%\n",
            "Epoch  30/50 | train loss 0.3167 acc 89.16% | val loss 0.4444 acc 85.18% | best val acc 85.18%\n",
            "Epoch  31/50 | train loss 0.3103 acc 89.29% | val loss 0.4489 acc 85.20% | best val acc 85.20%\n",
            "Epoch  32/50 | train loss 0.3004 acc 89.57% | val loss 0.4325 acc 85.62% | best val acc 85.62%\n",
            "Epoch  33/50 | train loss 0.2972 acc 89.59% | val loss 0.4498 acc 85.18% | best val acc 85.62%\n",
            "Epoch  34/50 | train loss 0.2901 acc 89.97% | val loss 0.4331 acc 85.68% | best val acc 85.68%\n",
            "Epoch  35/50 | train loss 0.2786 acc 90.44% | val loss 0.4283 acc 85.56% | best val acc 85.68%\n",
            "Epoch  36/50 | train loss 0.2722 acc 90.62% | val loss 0.4173 acc 86.18% | best val acc 86.18%\n",
            "Epoch  37/50 | train loss 0.2702 acc 90.57% | val loss 0.4168 acc 86.16% | best val acc 86.18%\n",
            "Epoch  38/50 | train loss 0.2627 acc 91.00% | val loss 0.4182 acc 86.32% | best val acc 86.32%\n",
            "Epoch  39/50 | train loss 0.2585 acc 91.09% | val loss 0.4128 acc 86.38% | best val acc 86.38%\n",
            "Epoch  40/50 | train loss 0.2554 acc 91.29% | val loss 0.4114 acc 86.52% | best val acc 86.52%\n",
            "Epoch  41/50 | train loss 0.2504 acc 91.32% | val loss 0.4131 acc 86.36% | best val acc 86.52%\n",
            "Epoch  42/50 | train loss 0.2432 acc 91.54% | val loss 0.4096 acc 86.58% | best val acc 86.58%\n",
            "Epoch  43/50 | train loss 0.2441 acc 91.48% | val loss 0.4107 acc 86.76% | best val acc 86.76%\n",
            "Epoch  44/50 | train loss 0.2393 acc 91.72% | val loss 0.4079 acc 86.64% | best val acc 86.76%\n",
            "Epoch  45/50 | train loss 0.2388 acc 91.79% | val loss 0.4048 acc 86.74% | best val acc 86.76%\n",
            "Epoch  46/50 | train loss 0.2369 acc 91.74% | val loss 0.4065 acc 86.62% | best val acc 86.76%\n",
            "Epoch  47/50 | train loss 0.2431 acc 91.61% | val loss 0.4063 acc 86.66% | best val acc 86.76%\n",
            "Epoch  48/50 | train loss 0.2381 acc 91.84% | val loss 0.4068 acc 86.52% | best val acc 86.76%\n",
            "Early stopping triggered at epoch 48.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
        "model3 = model3.to(device)  # Ensure model is on the correct device\n",
        "\n",
        "model3_train = train_with_early_stopping_adamw(\n",
        "    model3,\n",
        "    trainloader,\n",
        "    valloader,\n",
        "    device,\n",
        "    epochs=50,  ###epochs = 50\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-2,  ###\n",
        "    patience=3,   ###\n",
        "    save_path=\"best_resnet20_cifar10_adamw.pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bsP-pJeE_D0K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FINAL TEST | loss 0.4429 acc 85.85%\n"
          ]
        }
      ],
      "source": [
        "# Final test evaluation for ResNet-20 (Model 3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc = evaluate(model3_train, testloader, device, criterion)\n",
        "\n",
        "print(f\"FINAL TEST | loss {test_loss:.4f} acc {test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ResNet-20 Training Analysis\n",
        "\n",
        "**Training Dynamics:**\n",
        "- **Epochs trained**: 48 (early stopping triggered)\n",
        "- **Best validation accuracy**: 86.76% (epoch 43)\n",
        "- **Final test accuracy**: 85.85%\n",
        "- **Final test loss**: 0.4429\n",
        "- **Generalization gap**: 91.84% (train) - 85.85% (test) = **5.99%**\n",
        "\n",
        "**Learning Progression:**\n",
        "| Phase | Epochs | Train Acc | Val Acc | Observations |\n",
        "|-------|--------|-----------|---------|--------------|\n",
        "| Early | 1-10 | 41.3% -> 79.3% | 50.0% -> 77.9% | Rapid initial learning |\n",
        "| Middle | 11-30 | 80.2% -> 89.2% | 77.5% -> 85.2% | Steady improvement with skip connections |\n",
        "| Late | 31-48 | 89.3% -> 91.8% | 85.2% -> 86.8% | Slow refinement, early stopping triggered |\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Longest Training Duration**: ResNet-20 trained for 48 epochs before early stopping, significantly longer than SimpleCNN (16) and VGG (28). This indicates:\n",
        "   - The skip connections enable stable, gradual learning\n",
        "   - The model continues to improve slowly over many epochs\n",
        "   - The learning dynamics are different from plain CNNs\n",
        "\n",
        "2. **Largest Generalization Gap**: The 5.99% gap between train (91.84%) and test (85.85%) accuracy indicates:\n",
        "   - More overfitting compared to SimpleCNN (1.38%) and VGG (1.82%)\n",
        "   - The model memorizes training data more than the others\n",
        "   - May benefit from stronger regularization or different hyperparameters\n",
        "\n",
        "3. **Skip Connection Benefits**:\n",
        "   - Enables training of deeper networks (20 layers vs 5-6 in other models)\n",
        "   - Prevents vanishing gradient problem\n",
        "   - Identity shortcuts allow gradients to flow directly through the network\n",
        "   - The model learns residual functions (F(x) = H(x) - x) rather than direct mappings\n",
        "\n",
        "4. **Comparison with VGG**:\n",
        "   - ResNet-20 has 4x fewer parameters than VGG (272K vs 1.15M)\n",
        "   - Lower test accuracy (85.85% vs 87.92%) despite more layers\n",
        "   - Worse generalization gap (5.99% vs 1.82%)\n",
        "   - VGG's dropout regularization appears more effective for this dataset\n",
        "\n",
        "**Comparison with Other Models (Corrected):**\n",
        "| Metric | SimpleCNN | VGGSmallCIFAR | ResNet-20 |\n",
        "|--------|-----------|---------------|-----------|\n",
        "| Test Accuracy | 72.12% | **87.92%** | 85.85% |\n",
        "| Generalization Gap | **1.38%** | 1.82% | 5.99% |\n",
        "| Train Accuracy | 73.5% | 89.74% | 91.84% |\n",
        "| Parameters | 141K | 1.15M | 272K |\n",
        "\n",
        "**Key Distinction:**\n",
        "- **Test Accuracy** (absolute performance): 85.85% - second best\n",
        "- **Generalization Gap** (train-test difference): 5.99% - worst among all models (most overfitting)\n",
        "\n",
        "**Potential Improvements for ResNet-20:**\n",
        "- Add dropout layers to reduce overfitting\n",
        "- Use a higher learning rate (e.g., 0.1 with SGD) as originally proposed in the paper\n",
        "- Train without early stopping for full 200 epochs as in original CIFAR experiments\n",
        "- Add learning rate warmup\n",
        "- Use label smoothing or mixup augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO5XZDzL4-HO"
      },
      "source": [
        "# Summarize your findings here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TqbGqbEDB46"
      },
      "source": [
        "## Summary of Findings: Comparing CNN Architectures on CIFAR-10\n",
        "\n",
        "### Experimental Results\n",
        "\n",
        "| Model | Parameters | Epochs | Train Acc | Test Acc | Generalization Gap |\n",
        "|-------|-----------|--------|-----------|----------|-------------------|\n",
        "| SimpleCNN | 141,354 | 16 | 73.50% | 72.12% | **1.38%** (best) |\n",
        "| VGGSmallCIFAR | 1,149,770 | 28 | 89.74% | **87.92%** | 1.82% |\n",
        "| ResNet-20 | 272,474 | 48 | 91.84% | 85.85% | 5.99% (worst) |\n",
        "\n",
        "### Understanding the Metrics\n",
        "\n",
        "- **Test Accuracy**: Absolute performance on unseen data - higher is better\n",
        "- **Generalization Gap**: (Train Accuracy - Test Accuracy) - lower is better (less overfitting)\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "1. **SimpleCNN (72.12% test accuracy)**:\n",
        "   - **Best generalization** (1.38% gap) - the model does not overfit\n",
        "   - **Lowest test accuracy** - limited capacity prevents learning complex patterns\n",
        "   - This is a case of **underfitting**: the model generalizes well but lacks capacity\n",
        "\n",
        "2. **VGGSmallCIFAR (87.92% test accuracy - Best Performance)**:\n",
        "   - **Highest test accuracy** - best absolute performance on unseen data\n",
        "   - **Good generalization** (1.82% gap) - dropout regularization is effective\n",
        "   - Best balance between model capacity and regularization\n",
        "   - The additional parameters translate to better feature extraction without severe overfitting\n",
        "\n",
        "3. **ResNet-20 (85.85% test accuracy)**:\n",
        "   - **Worst generalization** (5.99% gap) - most overfitting among all models\n",
        "   - Despite having more layers (20 vs 6), it has fewer parameters than VGG\n",
        "   - The lack of dropout in ResNet architecture leads to more overfitting\n",
        "   - Skip connections enable training but don't prevent overfitting\n",
        "\n",
        "### Why VGG Achieved the Best Test Accuracy (NOT Best Generalization)\n",
        "\n",
        "| Factor | VGG Advantage |\n",
        "|--------|---------------|\n",
        "| Model Capacity | 1.15M params enables learning complex features |\n",
        "| Regularization | Dropout (0.25) after each block prevents overfitting |\n",
        "| Architecture | Progressive channel increase (64->128->256) captures hierarchical features |\n",
        "| Result | High capacity + good regularization = best test accuracy |\n",
        "\n",
        "### Generalization vs Test Accuracy Trade-off\n",
        "\n",
        "\n",
        "- **SimpleCNN**:  Low capacity  + Good regularization = Best generalization, Lowest test acc\n",
        "- **VGG**:        High capacity + Good regularization = Good generalization, Best test acc  \n",
        "- **ResNet-20**:  Med capacity  + No dropout          = Worst generalization, Medium test acc\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- **Best Test Accuracy**: VGGSmallCIFAR (87.92%) - highest performance on unseen data\n",
        "- **Best Generalization**: SimpleCNN (1.38% gap) - least overfitting, but underfits\n",
        "- **Worst Generalization**: ResNet-20 (5.99% gap) - most overfitting despite skip connections\n",
        "\n",
        "**Key Insight**: VGG achieves the best test accuracy NOT because it generalizes best, but because it has sufficient capacity to learn complex patterns while dropout prevents severe overfitting. SimpleCNN generalizes best but underfits due to limited capacity."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
